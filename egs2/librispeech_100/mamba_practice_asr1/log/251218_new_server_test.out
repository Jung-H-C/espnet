nohup: ignoring input
Using GPU: 0 (CUDA_VISIBLE_DEVICES=0)
Updating module_config in conf/train_asr.yaml...
Updated conf/train_asr.yaml with module_config:
  Configuration: ['I', 'I', 'I', 'C', 'M']
  Applied to 29 blocks
YAML file updated successfully.
2025-12-17T16:51:28 (asr.sh:285:main) ./asr.sh --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 6 --nbpe 1000 --max_wav_duration 30 --audio_format flac.ark --speed_perturb_factors 0.9 1.0 1.1 --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other --inference_asr_model valid.cer_ctc.ave_5best.pth --lm_train_text data/train_clean_100/text --bpe_train_text data/train_clean_100/text --stage 10 --stop_stage 13
2025-12-17T16:51:28 (asr.sh:566:main) Skipped stages:  6 7 8 9 14 15 
2025-12-17T16:51:28 (asr.sh:1191:main) Stage 10: ASR collect stats: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2025-12-17T16:51:28 (asr.sh:1242:main) Generate 'exp/asr_stats_raw_en_bpe1000_sp/run.sh'. You can resume the process from stage 10 using this script
2025-12-17T16:51:28 (asr.sh:1246:main) ASR collect-stats started... log: 'exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.*.log'
/home/gpuadmin/anaconda3/envs/junghc/bin/python3 /home/gpuadmin/espnet/espnet2/bin/aggregate_stats_dirs.py --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.1 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.2 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.3 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.4 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.5 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.6 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.7 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.8 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.9 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.10 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.11 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.12 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.13 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.14 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.15 --input_dir exp/asr_stats_raw_en_bpe1000_sp/logdir/stats.16 --output_dir exp/asr_stats_raw_en_bpe1000_sp
2025-12-17T16:53:25 (asr.sh:1310:main) Stage 11: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2025-12-17T16:53:25 (asr.sh:1409:main) Generate 'exp/asr_train_asr_raw_en_bpe1000_sp/run.sh'. You can resume the process from stage 11 using this script
2025-12-17T16:53:25 (asr.sh:1413:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe1000_sp/train.log'
2025-12-17T16:53:25 (asr.sh:1451:main) Use ESPnet trainer
2025-12-17 16:53:25,321 (launch:94) INFO: /home/gpuadmin/anaconda3/envs/junghc/bin/python3 /home/gpuadmin/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe1000_sp/train.log' --log exp/asr_train_asr_raw_en_bpe1000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe1000_sp/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_bpe1000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_bpe1000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,kaldi_ark --valid_shape_file exp/asr_stats_raw_en_bpe1000_sp/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe1000_sp --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe1000_sp/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark --train_shape_file exp/asr_stats_raw_en_bpe1000_sp/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe1000_sp/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe1000_sp/valid/text_shape.bpe
2025-12-17 16:53:25,341 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe1000_sp/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe1000_sp/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe1000_sp/train.log', '--gpu', '1', 'exp/asr_train_asr_raw_en_bpe1000_sp/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_bpe1000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_bpe1000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,kaldi_ark', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe1000_sp', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe1000_sp/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark', '--train_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100_sp/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/gpuadmin/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/home/gpuadmin/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe1000_sp/train.log ###################
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (16): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (17): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (18): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (19): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (20): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (21): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (22): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (23): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (24): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (25): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (26): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (27): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (28): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=1000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 22.10 M
    Number of trainable parameters: 22.10 M (100.0%)
    Size: 88.41 MB
    Type: torch.float32
[gpusystem] 2025-12-17 16:53:32,953 (abs_task:1459) INFO: Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 2.5e-07
    maximize: False
    weight_decay: 1e-06
)
[gpusystem] 2025-12-17 16:53:32,953 (abs_task:1460) INFO: Scheduler: WarmupLR(warmup_steps=8000)
[gpusystem] 2025-12-17 16:53:32,953 (abs_task:1469) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe1000_sp/config.yaml
[gpusystem] 2025-12-17 16:53:33,617 (abs_task:1885) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1419, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[gpusystem] 2025-12-17 16:53:33,617 (abs_task:1886) INFO: [train] mini-batch sizes summary: N-batch=1419, mean=60.3, min=26, max=337
[gpusystem] 2025-12-17 16:53:33,656 (read_text:31) INFO: keys_to_load is not None, only loading 85617 keys from dump/raw/train_clean_100_sp/text
[gpusystem] 2025-12-17 16:53:33,710 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[gpusystem] 2025-12-17 16:53:33,711 (abs_task:1910) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train_clean_100_sp/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/train_clean_100_sp/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f81dcf31d50>)
[gpusystem] 2025-12-17 16:53:33,732 (abs_task:1885) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=51, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[gpusystem] 2025-12-17 16:53:33,732 (abs_task:1886) INFO: [valid] mini-batch sizes summary: N-batch=51, mean=108.8, min=23, max=315
[gpusystem] 2025-12-17 16:53:33,734 (read_text:31) INFO: keys_to_load is not None, only loading 5551 keys from dump/raw/dev/text
[gpusystem] 2025-12-17 16:53:33,737 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[gpusystem] 2025-12-17 16:53:33,737 (abs_task:1910) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f81d82d0580>)
[gpusystem] 2025-12-17 16:53:33,742 (abs_task:1885) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=5551, batch_size=1, key_file=exp/asr_stats_raw_en_bpe1000_sp/valid/speech_shape, 
[gpusystem] 2025-12-17 16:53:33,742 (abs_task:1886) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[gpusystem] 2025-12-17 16:53:33,744 (read_text:31) INFO: keys_to_load is not None, only loading 3 keys from dump/raw/dev/text
[gpusystem] 2025-12-17 16:53:33,746 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[gpusystem] 2025-12-17 16:53:33,746 (abs_task:1910) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f81d82d1de0>)
/home/gpuadmin/espnet/espnet2/train/trainer.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
[gpusystem] 2025-12-17 16:53:34,297 (trainer:177) INFO: The training was resumed using exp/asr_train_asr_raw_en_bpe1000_sp/checkpoint.pth
[gpusystem] 2025-12-17 16:53:34,301 (trainer:336) INFO: 70/100epoch started
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn'
/home/gpuadmin/espnet/espnet2/train/trainer.py:638: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
/home/gpuadmin/espnet/espnet2/asr/espnet_model.py:403: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(self.autocast_frontend, dtype=autocast_type):
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py:196 in          │
│ _run_module_as_main                                                          │
│                                                                              │
│   193 │   main_globals = sys.modules["__main__"].__dict__                    │
│   194 │   if alter_argv:                                                     │
│   195 │   │   sys.argv[0] = mod_spec.origin                                  │
│ ❱ 196 │   return _run_code(code, main_globals, None,                         │
│   197 │   │   │   │   │    "__main__", mod_spec)                             │
│   198                                                                        │
│   199 def run_module(mod_name, init_globals=None,                            │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py:86 in _run_code │
│                                                                              │
│    83 │   │   │   │   │      __loader__ = loader,                            │
│    84 │   │   │   │   │      __package__ = pkg_name,                         │
│    85 │   │   │   │   │      __spec__ = mod_spec)                            │
│ ❱  86 │   exec(code, run_globals)                                            │
│    87 │   return run_globals                                                 │
│    88                                                                        │
│    89 def _run_module_code(code, init_globals=None,                          │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/bin/asr_train.py:23 in <module>                │
│                                                                              │
│   20                                                                         │
│   21                                                                         │
│   22 if __name__ == "__main__":                                              │
│ ❱ 23 │   main()                                                              │
│   24                                                                         │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/bin/asr_train.py:19 in main                    │
│                                                                              │
│   16 │   │   │   │   > conf/train_asr.yaml                                   │
│   17 │   │   % python asr_train.py --config conf/train_asr.yaml              │
│   18 │   """                                                                 │
│ ❱ 19 │   ASRTask.main(cmd=cmd)                                               │
│   20                                                                         │
│   21                                                                         │
│   22 if __name__ == "__main__":                                              │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/tasks/abs_task.py:1281 in main                 │
│                                                                              │
│   1278 │   │   # "distributed" is decided using the other command args       │
│   1279 │   │   resolve_distributed_mode(args)                                │
│   1280 │   │   if not args.distributed or not args.multiprocessing_distribut │
│ ❱ 1281 │   │   │   cls.main_worker(args)                                     │
│   1282 │   │                                                                 │
│   1283 │   │   else:                                                         │
│   1284 │   │   │   assert args.ngpu > 1, args.ngpu                           │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/tasks/abs_task.py:1638 in main_worker          │
│                                                                              │
│   1635 │   │   │   │   │   distributed_option.init_deepspeed()               │
│   1636 │   │   │                                                             │
│   1637 │   │   │   trainer_options = cls.trainer.build_options(args)         │
│ ❱ 1638 │   │   │   cls.trainer.run(                                          │
│   1639 │   │   │   │   model=model,                                          │
│   1640 │   │   │   │   optimizers=optimizers,                                │
│   1641 │   │   │   │   schedulers=schedulers,                                │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/train/trainer.py:343 in run                    │
│                                                                              │
│   340 │   │   │   # 1. Train and validation for one-epoch                    │
│   341 │   │   │   torch.cuda.empty_cache()                                   │
│   342 │   │   │   with reporter.observe("train") as sub_reporter:            │
│ ❱ 343 │   │   │   │   all_steps_are_invalid = cls.train_one_epoch(           │
│   344 │   │   │   │   │   model=dp_model,                                    │
│   345 │   │   │   │   │   optimizers=optimizers,                             │
│   346 │   │   │   │   │   schedulers=schedulers,                             │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/train/trainer.py:643 in train_one_epoch        │
│                                                                              │
│   640 │   │   │   │   **autocast_args,                                       │
│   641 │   │   │   ):                                                         │
│   642 │   │   │   │   with reporter.measure_time("forward_time"):            │
│ ❱ 643 │   │   │   │   │   retval = model(**batch)                            │
│   644 │   │   │   │   │                                                      │
│   645 │   │   │   │   │   # Note(kamo):                                      │
│   646 │   │   │   │   │   # Supporting two patterns for the returned value f │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1775 in _wrapped_call_impl                                  │
│                                                                              │
│   1772 │   │   if self._compiled_call_impl is not None:                      │
│   1773 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: │
│   1774 │   │   else:                                                         │
│ ❱ 1775 │   │   │   return self._call_impl(*args, **kwargs)                   │
│   1776 │                                                                     │
│   1777 │   # torchrec tests the code consistency with the following code     │
│   1778 │   # fmt: off                                                        │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1786 in _call_impl                                          │
│                                                                              │
│   1783 │   │   if not (self._backward_hooks or self._backward_pre_hooks or s │
│   1784 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hoo │
│   1785 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1786 │   │   │   return forward_call(*args, **kwargs)                      │
│   1787 │   │                                                                 │
│   1788 │   │   result = None                                                 │
│   1789 │   │   called_always_called_hooks = set()                            │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/espnet_model.py:257 in forward             │
│                                                                              │
│   254 │   │   text = text[:, : text_lengths.max()]                           │
│   255 │   │                                                                  │
│   256 │   │   # 1. Encoder                                                   │
│ ❱ 257 │   │   encoder_out, encoder_out_lens = self.encode(speech, speech_len │
│   258 │   │   intermediate_outs = None                                       │
│   259 │   │   if isinstance(encoder_out, tuple):                             │
│   260 │   │   │   intermediate_outs = encoder_out[1]                         │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/espnet_model.py:433 in encode              │
│                                                                              │
│   430 │   │   │   │   │   feats, feats_lengths, ctc=self.ctc                 │
│   431 │   │   │   │   )                                                      │
│   432 │   │   │   else:                                                      │
│ ❱ 433 │   │   │   │   encoder_out, encoder_out_lens, _ = self.encoder(feats, │
│   434 │   │                                                                  │
│   435 │   │   intermediate_outs = None                                       │
│   436 │   │   if isinstance(encoder_out, tuple):                             │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1775 in _wrapped_call_impl                                  │
│                                                                              │
│   1772 │   │   if self._compiled_call_impl is not None:                      │
│   1773 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: │
│   1774 │   │   else:                                                         │
│ ❱ 1775 │   │   │   return self._call_impl(*args, **kwargs)                   │
│   1776 │                                                                     │
│   1777 │   # torchrec tests the code consistency with the following code     │
│   1778 │   # fmt: off                                                        │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1786 in _call_impl                                          │
│                                                                              │
│   1783 │   │   if not (self._backward_hooks or self._backward_pre_hooks or s │
│   1784 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hoo │
│   1785 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1786 │   │   │   return forward_call(*args, **kwargs)                      │
│   1787 │   │                                                                 │
│   1788 │   │   result = None                                                 │
│   1789 │   │   called_always_called_hooks = set()                            │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/encoder/mamba_encoder.py:338 in forward    │
│                                                                              │
│   335 │   │   if len(self.interctc_layer_idx) == 0:                          │
│   336 │   │   │   for layer_idx, encoder_layer in enumerate(self.encoders):  │
│   337 │   │   │   │   # MambaCellBlock forward: (xs_pad, masks) -> (xs_pad,  │
│ ❱ 338 │   │   │   │   xs_pad, masks = encoder_layer(xs_pad, masks)           │
│   339 │   │   │   │                                                          │
│   340 │   │   │   │   if return_all_hs:                                      │
│   341 │   │   │   │   │   if isinstance(xs_pad, tuple):                      │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1775 in _wrapped_call_impl                                  │
│                                                                              │
│   1772 │   │   if self._compiled_call_impl is not None:                      │
│   1773 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: │
│   1774 │   │   else:                                                         │
│ ❱ 1775 │   │   │   return self._call_impl(*args, **kwargs)                   │
│   1776 │                                                                     │
│   1777 │   # torchrec tests the code consistency with the following code     │
│   1778 │   # fmt: off                                                        │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1786 in _call_impl                                          │
│                                                                              │
│   1783 │   │   if not (self._backward_hooks or self._backward_pre_hooks or s │
│   1784 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hoo │
│   1785 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1786 │   │   │   return forward_call(*args, **kwargs)                      │
│   1787 │   │                                                                 │
│   1788 │   │   result = None                                                 │
│   1789 │   │   called_always_called_hooks = set()                            │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/state_spaces/mamba_blocks.py:448 in        │
│ forward                                                                      │
│                                                                              │
│   445 │   │   │   if self.normalize_before:                                  │
│   446 │   │   │   │   x = self._apply_pre_norm(x, pre_norm)                  │
│   447 │   │   │   # Apply module                                             │
│ ❱ 448 │   │   │   x, masks = module(x, masks)                                │
│   449 │   │   │   # Residual connection: add residual before module to outpu │
│   450 │   │   │   if use_residual:                                           │
│   451 │   │   │   │   x = residual + stoch_layer_coeff * x                   │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1775 in _wrapped_call_impl                                  │
│                                                                              │
│   1772 │   │   if self._compiled_call_impl is not None:                      │
│   1773 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: │
│   1774 │   │   else:                                                         │
│ ❱ 1775 │   │   │   return self._call_impl(*args, **kwargs)                   │
│   1776 │                                                                     │
│   1777 │   # torchrec tests the code consistency with the following code     │
│   1778 │   # fmt: off                                                        │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1786 in _call_impl                                          │
│                                                                              │
│   1783 │   │   if not (self._backward_hooks or self._backward_pre_hooks or s │
│   1784 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hoo │
│   1785 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1786 │   │   │   return forward_call(*args, **kwargs)                      │
│   1787 │   │                                                                 │
│   1788 │   │   result = None                                                 │
│   1789 │   │   called_always_called_hooks = set()                            │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/state_spaces/mamba_blocks.py:214 in        │
│ forward                                                                      │
│                                                                              │
│   211 │   │   """                                                            │
│   212 │   │   # BiMamba doesn't use masks in its forward pass                │
│   213 │   │   # Apply masks after if needed                                  │
│ ❱ 214 │   │   out = self.mamba(x)                                            │
│   215 │   │   if masks is not None:                                          │
│   216 │   │   │   # masks shape: (batch, 1, seq_len), need to transpose for  │
│   217 │   │   │   out = out.masked_fill(~masks.squeeze(1).unsqueeze(-1), 0.0 │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1775 in _wrapped_call_impl                                  │
│                                                                              │
│   1772 │   │   if self._compiled_call_impl is not None:                      │
│   1773 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: │
│   1774 │   │   else:                                                         │
│ ❱ 1775 │   │   │   return self._call_impl(*args, **kwargs)                   │
│   1776 │                                                                     │
│   1777 │   # torchrec tests the code consistency with the following code     │
│   1778 │   # fmt: off                                                        │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m │
│ odules/module.py:1786 in _call_impl                                          │
│                                                                              │
│   1783 │   │   if not (self._backward_hooks or self._backward_pre_hooks or s │
│   1784 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hoo │
│   1785 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1786 │   │   │   return forward_call(*args, **kwargs)                      │
│   1787 │   │                                                                 │
│   1788 │   │   result = None                                                 │
│   1789 │   │   called_always_called_hooks = set()                            │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/state_spaces/bimamba.py:228 in forward     │
│                                                                              │
│   225 │   │   │   │   )                                                      │
│   226 │   │   │   elif self.bimamba_type == "v2":                            │
│   227 │   │   │   │   A_b = -torch.exp(self.A_b_log.float())                 │
│ ❱ 228 │   │   │   │   out = mamba_inner_fn_no_out_proj(                      │
│   229 │   │   │   │   │   xz,                                                │
│   230 │   │   │   │   │   self.conv1d.weight,                                │
│   231 │   │   │   │   │   self.conv1d.bias,                                  │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/state_spaces/selective_scan_interface.py:6 │
│ 37 in mamba_inner_fn_no_out_proj                                             │
│                                                                              │
│   634 │   A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,      │
│   635 │   C_proj_bias=None, delta_softplus=True                              │
│   636 ):                                                                     │
│ ❱ 637 │   return MambaInnerFnNoOutProj.apply(xz, conv1d_weight, conv1d_bias, │
│   638 │   │   │   │   │   │   │     A, B, C, D, delta_bias, B_proj_bias, C_p │
│   639                                                                        │
│   640                                                                        │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/auto │
│ grad/function.py:581 in apply                                                │
│                                                                              │
│   578 │   │   if not torch._C._are_functorch_transforms_active():            │
│   579 │   │   │   # See NOTE: [functorch vjp and autograd interaction]       │
│   580 │   │   │   args = _functorch.utils.unwrap_dead_wrappers(args)         │
│ ❱ 581 │   │   │   return super().apply(*args, **kwargs)  # type: ignore[misc │
│   582 │   │                                                                  │
│   583 │   │   if not is_setup_ctx_defined:                                   │
│   584 │   │   │   raise RuntimeError(                                        │
│                                                                              │
│ /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/amp/ │
│ autocast_mode.py:527 in decorate_fwd                                         │
│                                                                              │
│   524 │   │   args[0]._dtype = torch.get_autocast_dtype(device_type)         │
│   525 │   │   if cast_inputs is None:                                        │
│   526 │   │   │   args[0]._fwd_used_autocast = torch.is_autocast_enabled(dev │
│ ❱ 527 │   │   │   return fwd(*args, **kwargs)                                │
│   528 │   │   else:                                                          │
│   529 │   │   │   autocast_context = torch.is_autocast_enabled(device_type)  │
│   530 │   │   │   args[0]._fwd_used_autocast = False                         │
│                                                                              │
│ /home/gpuadmin/espnet/espnet2/asr/state_spaces/selective_scan_interface.py:1 │
│ 82 in forward                                                                │
│                                                                              │
│   179 │   │   conv1d_weight = rearrange(conv1d_weight, "d 1 w -> d w")       │
│   180 │   │   x, z = xz.chunk(2, dim=1)                                      │
│   181 │   │   conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not N │
│ ❱ 182 │   │   conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_we │
│   183 │   │   # We're being very careful here about the layout, to avoid ext │
│   184 │   │   # We want delta to have d as the slowest moving dimension      │
│   185 │   │   # and L as the fastest moving dimension, since those are what  │
╰──────────────────────────────────────────────────────────────────────────────╯
TypeError: causal_conv1d_fwd(): incompatible function arguments. The following 
argument types are supported:
    1. (arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor | None, arg3:
torch.Tensor | None, arg4: torch.Tensor | None, arg5: torch.Tensor, arg6: 
torch.Tensor | None, arg7: bool) -> None

Invoked with: tensor([[[-2.7539e-01,  2.8320e-01,  5.7373e-03,  ...,  
1.3770e-01,
           4.1748e-02, -1.0498e-01],
         [-1.7188e-01,  9.2285e-02,  1.0547e+00,  ...,  9.6875e-01,
           2.4512e-01,  4.3164e-01],
         [ 1.2656e+00,  1.4375e+00,  1.5234e+00,  ...,  8.2812e-01,
           7.3047e-01,  9.6094e-01],
         ...,
         [-2.8125e-01, -1.7383e-01, -3.6377e-02,  ..., -3.8477e-01,
          -1.9727e-01,  1.0742e-01],
         [-1.3281e+00, -2.1719e+00, -1.6797e+00,  ..., -2.5469e+00,
          -2.3281e+00, -2.5312e+00],
         [-7.6562e-01, -6.0156e-01,  8.7891e-01,  ...,  3.1445e-01,
           5.4688e-01,  1.1719e+00]],

        [[-4.1016e-01, -5.2734e-01, -4.5703e-01,  ..., -8.5938e-02,
           1.6699e-01,  2.8516e-01],
         [ 1.4297e+00, -9.0625e-01, -8.2031e-01,  ...,  1.4062e+00,
           1.0547e+00,  1.2422e+00],
         [ 1.1279e-01, -8.6426e-02,  8.3008e-02,  ...,  7.6953e-01,
           1.7188e+00,  8.6328e-01],
         ...,
         [-2.5000e-01, -1.1133e-01,  2.3047e-01,  ..., -7.5000e-01,
          -4.9023e-01, -1.1875e+00],
         [-1.7812e+00, -6.0938e-01, -4.0430e-01,  ..., -1.8672e+00,
          -1.6641e+00, -1.0625e+00],
         [-4.6094e-01, -5.9570e-02, -7.6172e-02,  ...,  9.3750e-01,
          -1.1133e-01, -1.7031e+00]],

        [[-9.3359e-01, -6.5234e-01, -2.3926e-01,  ...,  8.4473e-02,
          -9.4238e-02, -3.4766e-01],
         [-6.5430e-02,  1.4551e-01,  1.2012e-01,  ...,  5.9766e-01,
           8.3594e-01,  6.6797e-01],
         [ 3.8867e-01,  6.5625e-01,  8.6328e-01,  ...,  5.8594e-01,
           2.1973e-01,  7.8125e-01],
         ...,
         [-1.3203e+00, -9.4922e-01, -8.9062e-01,  ..., -8.9844e-01,
          -2.8320e-01, -4.1797e-01],
         [-1.2422e+00, -1.1641e+00, -2.1875e+00,  ..., -1.5703e+00,
          -1.6250e+00, -1.0078e+00],
         [ 9.4922e-01,  8.2031e-01,  5.9375e-01,  ...,  7.5391e-01,
           1.0547e+00,  7.8125e-01]],

        ...,

        [[ 5.0781e-01,  2.2188e+00,  7.6562e-01,  ..., -7.6660e-02,
           1.9727e-01,  1.5547e+00],
         [ 3.0273e-01, -4.9609e-01,  1.3672e-01,  ..., -4.2188e-01,
          -7.8125e-01,  1.1172e+00],
         [ 1.0469e+00,  1.1094e+00,  1.2969e+00,  ...,  1.5469e+00,
           7.7344e-01, -9.4922e-01],
         ...,
         [-1.4219e+00, -6.4844e-01, -9.3359e-01,  ..., -4.3359e-01,
          -2.3828e-01, -3.0273e-01],
         [-2.6875e+00, -2.2500e+00, -3.5156e+00,  ..., -2.2656e+00,
          -2.1562e+00, -7.6953e-01],
         [-1.0859e+00, -7.6172e-01, -3.0469e-01,  ...,  4.4922e-02,
          -3.9482e-04,  3.4180e-01]],

        [[-1.4941e-01,  3.0664e-01,  6.9531e-01,  ...,  3.6328e-01,
           3.7891e-01, -1.6992e-01],
         [-1.0234e+00, -9.1797e-01, -3.2227e-01,  ..., -4.8438e-01,
          -1.1328e+00,  2.9663e-02],
         [ 1.3281e+00,  1.4609e+00,  1.2969e+00,  ...,  8.3203e-01,
           1.6797e+00, -4.0625e-01],
         ...,
         [-2.8320e-01, -2.2339e-02, -2.0264e-02,  ...,  5.1953e-01,
          -7.2754e-02,  1.3281e-01],
         [-1.1094e+00, -1.2969e+00, -1.5000e+00,  ..., -7.5391e-01,
          -9.6875e-01, -5.6250e-01],
         [ 1.7871e-01,  5.4297e-01,  8.5156e-01,  ...,  4.0234e-01,
           4.9805e-01,  1.2344e+00]],

        [[-3.1836e-01,  1.9727e-01, -3.0273e-01,  ..., -1.9727e-01,
          -7.2266e-02,  5.6250e-01],
         [ 1.0938e+00,  4.9219e-01,  1.5859e+00,  ...,  1.0859e+00,
           1.6172e+00,  1.8516e+00],
         [ 3.3398e-01,  7.3047e-01,  2.5000e-01,  ...,  6.6406e-01,
           5.9766e-01, -4.4336e-01],
         ...,
         [-4.7852e-01, -6.8359e-01, -6.0547e-01,  ..., -9.8047e-01,
          -2.2266e-01,  2.1387e-01],
         [-9.0625e-01, -3.8477e-01, -9.1797e-01,  ..., -6.8359e-01,
          -5.0000e-01,  1.7500e+00],
         [ 4.8242e-01,  2.7539e-01,  3.7305e-01,  ...,  6.4453e-02,
           1.0781e+00,  1.0781e+00]]], device='cuda:0', dtype=torch.bfloat16,
       requires_grad=True), tensor([[ 0.0487,  0.1979,  0.1221,  0.0152],
        [ 0.0782,  0.2675, -0.0997,  0.0844],
        [-0.0898,  0.0365,  0.0448,  0.1911],
        ...,
        [ 0.2344, -0.2125,  0.1284,  0.1966],
        [ 0.0980, -0.1778,  0.1908,  0.2904],
        [ 0.0641,  0.1687,  0.1263, -0.0216]], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([ 3.7693e-01,  4.3364e-01, -1.2290e+00,  1.8220e-01, -4.4395e-01,
        -2.4082e-01,  4.3818e-02, -2.4251e-01,  4.6918e-01,  3.0050e-01,
        -3.9460e-01,  7.9675e-02, -2.8780e-01,  3.3119e-01,  1.6079e-01,
         2.4864e-01, -2.4634e-01,  2.9254e-01,  7.6679e-02,  4.2115e-01,
        -2.3156e-01,  1.9180e-01,  6.3158e-01, -7.2491e-01,  1.0937e-01,
        -6.4980e-02,  8.4075e-02, -2.7123e-01, -3.2229e-01,  3.1365e-02,
        -1.7018e-01, -8.8581e-02, -1.4814e-02,  1.3084e-01, -4.4415e-02,
        -2.7745e-01, -7.9081e-01, -3.5119e-01,  2.4082e-01, -4.4633e-01,
        -5.8390e-02, -1.4092e-01,  3.2467e-01, -3.8665e-01,  4.7381e-01,
        -3.5946e-02,  3.3045e-01,  1.6763e-01, -3.1102e-01, -1.4315e-01,
         8.7211e-02, -3.8066e-01, -5.3512e-01,  3.4850e-02, -4.2577e-01,
        -7.9731e-02, -4.4513e-01,  5.8474e-01,  2.1901e-01, -3.7869e-01,
        -2.3407e-01,  2.1470e-01, -1.1912e-01, -6.4799e-01,  1.5639e-01,
        -2.6218e-02, -4.1935e-01,  2.5734e-01, -6.8411e-01,  2.7451e-01,
        -5.7460e-02,  1.4620e-01, -7.2968e-02, -5.3359e-02,  5.1650e-02,
        -4.9580e-01, -1.0466e-01,  7.8506e-02, -1.7525e-01,  4.7667e-02,
         2.2557e-01,  2.8311e-01, -8.6699e-01,  2.4955e-01, -3.9430e-01,
        -2.8761e-02,  2.1013e-01,  3.5540e-01, -3.7715e-01,  1.1380e-01,
         3.1173e-01,  1.9328e-01, -2.9779e-01, -6.2997e-01, -1.2366e-01,
         2.5584e-01, -6.1176e-01, -5.0620e-01, -3.6969e-01,  1.8672e-02,
         4.2887e-01, -4.4408e-02, -3.8595e-01,  3.8643e-01,  3.0475e-01,
         2.7548e-01, -2.7997e-01, -1.1280e-01,  1.1225e-01,  2.8082e-01,
        -2.1347e-01,  6.1496e-02,  1.4859e-01,  8.2569e-03, -3.8695e-01,
         2.7545e-01, -7.2727e-01,  8.6990e-02, -8.0534e-02,  5.5940e-01,
        -3.0250e-01, -1.1881e-02, -6.5192e-02,  8.1735e-04,  4.9105e-01,
         9.3904e-02,  1.1165e-01,  6.8577e-02, -2.0010e-01,  3.6131e-01,
        -3.8151e-01, -3.7516e-01, -4.1715e-01, -5.0713e-01, -3.0902e-01,
         2.0340e-01, -9.8820e-01, -5.7147e-02, -2.0555e-01,  5.1786e-01,
         1.9923e-01, -5.3492e-01, -7.9234e-01,  2.6826e-01, -8.7465e-03,
        -2.5752e-02,  9.6158e-03, -8.0155e-03, -5.6023e-01,  4.1297e-01,
         3.9052e-03, -1.4151e-01,  2.7262e-01, -1.9623e-01, -2.1346e-02,
        -8.3928e-01, -3.8719e-01, -2.6614e-01,  1.3926e-01, -1.4223e-01,
         3.2202e-01,  1.4653e-01,  3.6457e-01, -2.2869e-01, -2.3198e-01,
         1.3487e-01,  5.9213e-02, -1.8193e-01,  1.6838e-01,  4.5077e-01,
        -2.5270e-01, -2.0170e-01,  1.7157e-01, -2.9093e-01,  8.1581e-03,
        -3.7001e-01,  1.2434e-01,  9.4451e-02, -1.4984e-01,  1.5435e-01,
        -8.2443e-01,  9.3856e-02, -1.3342e-01,  2.9087e-01,  3.9432e-01,
         9.7064e-02, -8.4776e-01,  9.3284e-02, -4.3166e-01, -3.7275e-01,
        -1.1519e-01, -7.8803e-01,  2.8655e-01, -9.1426e-01, -5.1257e-01,
        -8.7530e-01,  1.1124e-01,  3.4342e-01, -4.4761e-01, -3.2470e-01,
        -4.2164e-01, -1.6826e-01,  3.7255e-01,  2.1113e-01,  1.5390e-01,
         9.3411e-02,  3.8957e-01,  1.1897e-01, -1.3432e-01, -2.5483e-01,
        -5.0271e-01, -3.8367e-02, -1.9376e-01,  1.9218e-01,  3.4871e-01,
        -3.8091e-01,  4.3277e-01,  3.1717e-01,  7.5364e-03, -3.7090e-01,
         4.6869e-01, -3.2554e-01,  2.7481e-01,  6.3273e-02, -1.2150e-01,
        -2.6977e-01, -7.0579e-02,  3.4881e-02, -4.1855e-01,  2.2728e-01,
         3.7989e-01,  1.0634e-02,  4.3057e-01, -3.2026e-02,  6.7923e-02,
        -4.4745e-01,  1.5171e-01,  2.4932e-01, -1.1994e-01, -4.0165e-01,
        -1.2602e-01, -4.8460e-01, -9.3507e-01, -8.9312e-02, -2.6066e-01,
         1.1189e-01,  1.5761e-01, -2.2429e-01, -2.0926e-02, -3.6213e-02,
        -3.8632e-01, -5.9375e-02,  3.8874e-01,  4.2322e-01, -1.1842e-02,
        -1.1443e-01, -1.9005e-01, -4.5956e-01, -9.8046e-01, -7.7564e-03,
        -1.2704e-01,  3.3731e-01, -2.3001e-01,  1.2929e-01,  5.7226e-01,
        -1.5828e-01, -3.4048e-01, -2.6353e-01,  2.6607e-01,  3.1483e-01,
        -6.0982e-02, -2.5883e-02, -3.0184e-01, -2.4092e-01,  3.7147e-02,
        -3.4980e-01,  2.7534e-03,  1.0081e-01, -7.5301e-02,  3.9217e-01,
        -2.4898e-01,  5.2712e-01, -3.4631e-01, -3.7752e-01, -1.8360e-01,
        -1.3131e-01,  1.0068e-01,  1.1384e-01,  1.4849e-01, -9.2128e-01,
        -7.1079e-02, -2.0511e-02, -2.8436e-01,  7.3478e-02, -8.5680e-01,
        -2.6156e-01,  3.5041e-01, -3.9637e-02,  2.3086e-01, -3.7197e-03,
         2.9740e-01, -6.3313e-02,  5.2196e-01, -5.7459e-01,  4.6526e-01,
        -1.0622e-02, -7.8868e-01, -3.6449e-02, -5.4389e-01, -4.2398e-01,
         4.3393e-01, -2.6450e-01,  1.1320e-01, -2.0953e-01,  2.5744e-01,
        -2.7922e-01, -1.5344e-01,  5.5332e-02, -3.3842e-01, -4.1390e-01,
        -6.7714e-01,  2.4009e-01,  3.5197e-01, -7.9727e-04, -8.5310e-01,
        -1.0384e-01, -1.1090e-01, -1.4834e-01, -9.8369e-02,  3.2289e-01,
        -6.5346e-01,  4.2733e-02, -3.0657e-02, -7.9126e-02,  2.7442e-01,
        -8.2067e-02,  1.1175e-01, -1.4326e-01, -1.2846e-03, -3.7359e-01,
         8.3291e-02, -1.3111e-01, -1.8864e-02,  2.1364e-01,  3.5754e-01,
         4.8007e-01, -2.7841e-02, -3.1494e-02, -2.2639e-01, -6.3663e-01,
         3.8918e-01, -4.3688e-02, -3.9558e-01, -4.1677e-01, -5.5365e-01,
        -1.7608e-01, -1.6352e-01, -8.4494e-02, -2.0077e-01, -3.3898e-01,
        -2.5723e-01,  4.7446e-01,  1.0993e-01,  2.1084e-01,  8.3357e-02,
         4.5072e-01, -1.1739e-01,  2.9228e-01, -7.2033e-02,  3.7094e-01,
         2.7086e-01,  1.9462e-02,  1.0348e-02, -3.5813e-01, -3.0308e-01,
        -7.4968e-02, -3.0088e-01, -5.1430e-01,  3.4433e-01, -2.9847e-01,
         2.1094e-01,  4.7786e-01, -3.7729e-01,  2.3810e-01, -3.2795e-01,
         5.8741e-01,  2.3792e-01,  3.6020e-01, -5.8625e-01, -5.5208e-03,
        -3.9951e-01,  1.8839e-02, -2.2679e-01, -2.3637e-01,  4.2221e-01,
        -5.2463e-01, -6.4408e-03,  1.4881e-01,  1.3388e-01,  3.8686e-01,
        -8.7514e-02,  6.3518e-02,  2.1443e-01,  5.1564e-02, -3.7308e-01,
         2.2498e-01, -1.6176e-01,  4.1735e-01,  1.4051e-01, -4.7802e-01,
         2.0269e-01,  1.2981e-02, -7.3491e-01,  5.3626e-01,  9.0687e-02,
        -2.2747e-01, -5.9551e-01, -2.2952e-01, -6.6511e-01,  5.7006e-01,
        -4.5777e-01, -3.9476e-01,  3.1092e-02,  8.1623e-02,  3.2292e-02,
        -1.7145e-01,  4.2411e-01, -3.4105e-01, -4.0353e-01, -4.3272e-01,
        -6.7145e-01,  4.6485e-02, -2.5065e-01, -6.0207e-02,  5.1674e-02,
        -1.8045e-02, -1.0176e-01, -3.5795e-01,  8.5327e-02,  1.3737e-01,
         3.3692e-01, -8.0286e-01, -6.5484e-01, -2.3123e-01, -2.9825e-02,
        -7.7179e-01, -1.9212e-01,  1.5805e-01, -3.1514e-01,  3.6337e-01,
         1.1915e-02, -1.1097e-01, -2.7254e-01,  1.0142e-01, -5.9673e-01,
         2.7556e-01,  1.5110e-01,  5.1301e-01,  1.5934e-01,  7.8081e-02,
        -2.8462e-01, -6.7234e-01, -6.9764e-01,  4.7509e-02, -1.3190e-01,
         4.4131e-01,  2.9315e-02, -3.7155e-01,  2.8394e-01,  1.3320e-01,
        -3.2562e-01, -1.0861e-01,  5.6073e-02, -6.8688e-01,  3.2915e-01,
         2.8197e-01,  5.1356e-02,  3.6508e-01, -1.0134e-01, -3.6978e-01,
         2.6750e-01, -4.9514e-03, -5.9019e-01, -4.3052e-01, -1.8964e-02,
         2.9516e-02,  1.4222e-01,  5.4057e-02,  3.0299e-01, -5.3823e-01,
         3.4658e-01,  2.8833e-01, -2.6609e-01,  1.2081e-01, -2.4405e-01,
        -9.0883e-02, -8.1141e-02,  3.4806e-01, -4.4118e-01, -5.3559e-02,
        -3.2504e-01, -3.6931e-01, -1.0137e-01,  4.0843e-01,  1.5577e-01,
        -4.2397e-01,  4.3416e-02,  2.9235e-01,  4.0963e-01,  2.5303e-01,
         2.2103e-01,  7.1483e-02], device='cuda:0', requires_grad=True), None, 
True
# Accounting: time=12 threads=1
# Ended (code 1) at Wed Dec 17 16:53:37 UTC 2025, elapsed time 12 seconds

