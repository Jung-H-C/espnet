nohup: ignoring input
Using GPU: 1 (CUDA_VISIBLE_DEVICES=1)
Updating module_config in conf/train_asr.yaml...
Updated conf/train_asr.yaml with module_config:
  Configuration: ['I', 'I', 'I', 'C', 'M']
  Applied to 29 blocks
YAML file updated successfully.
2025-12-19T05:12:22 (asr.sh:285:main) ./asr.sh --lang en --ngpu 1 --nj 16 --gpu_inference true --inference_nj 6 --nbpe 1000 --max_wav_duration 30 --audio_format flac.ark --speed_perturb_factors 0.9 1.0 1.1 --feats_type raw --use_lm false --asr_config conf/train_asr.yaml --inference_config conf/decode_asr.yaml --train_set train_clean_100 --valid_set dev --test_sets test_clean test_other --inference_asr_model valid.cer_ctc.ave_5best.pth --lm_train_text data/train_clean_100/text --bpe_train_text data/train_clean_100/text --stage 11 --stop_stage 13
2025-12-19T05:12:22 (asr.sh:566:main) Skipped stages:  6 7 8 9 14 15 
2025-12-19T05:12:22 (asr.sh:1310:main) Stage 11: ASR Training: train_set=dump/raw/train_clean_100_sp, valid_set=dump/raw/dev
2025-12-19T05:12:22 (asr.sh:1409:main) Generate 'exp/asr_train_asr_raw_en_bpe1000_sp/run.sh'. You can resume the process from stage 11 using this script
2025-12-19T05:12:22 (asr.sh:1413:main) ASR training started... log: 'exp/asr_train_asr_raw_en_bpe1000_sp/train.log'
2025-12-19T05:12:22 (asr.sh:1451:main) Use ESPnet trainer
2025-12-19 05:12:22,928 (launch:94) INFO: /home/gpuadmin/anaconda3/envs/junghc/bin/python3 /home/gpuadmin/espnet/espnet2/bin/launch.py --cmd 'run.pl --name exp/asr_train_asr_raw_en_bpe1000_sp/train.log' --log exp/asr_train_asr_raw_en_bpe1000_sp/train.log --ngpu 1 --num_nodes 1 --init_file_prefix exp/asr_train_asr_raw_en_bpe1000_sp/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel data/en_token_list/bpe_bpe1000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_bpe1000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/dev/wav.scp,speech,kaldi_ark --valid_shape_file exp/asr_stats_raw_en_bpe1000_sp/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir exp/asr_train_asr_raw_en_bpe1000_sp --config conf/train_asr.yaml --frontend_conf fs=16k --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats_raw_en_bpe1000_sp/train/feats_stats.npz --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark --train_shape_file exp/asr_stats_raw_en_bpe1000_sp/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type dump/raw/train_clean_100_sp/text,text,text --train_shape_file exp/asr_stats_raw_en_bpe1000_sp/train/text_shape.bpe --valid_data_path_and_name_and_type dump/raw/dev/text,text,text --valid_shape_file exp/asr_stats_raw_en_bpe1000_sp/valid/text_shape.bpe
2025-12-19 05:12:22,942 (launch:348) INFO: log file: exp/asr_train_asr_raw_en_bpe1000_sp/train.log
run.pl: job failed, log is in exp/asr_train_asr_raw_en_bpe1000_sp/train.log
Command '['run.pl', '--name', 'exp/asr_train_asr_raw_en_bpe1000_sp/train.log', '--gpu', '1', 'exp/asr_train_asr_raw_en_bpe1000_sp/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'data/en_token_list/bpe_bpe1000/bpe.model', '--token_type', 'bpe', '--token_list', 'data/en_token_list/bpe_bpe1000/tokens.txt', '--non_linguistic_symbols', 'none', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', 'dump/raw/dev/wav.scp,speech,kaldi_ark', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', 'exp/asr_train_asr_raw_en_bpe1000_sp', '--config', 'conf/train_asr.yaml', '--frontend_conf', 'fs=16k', '--normalize=global_mvn', '--normalize_conf', 'stats_file=exp/asr_stats_raw_en_bpe1000_sp/train/feats_stats.npz', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100_sp/wav.scp,speech,kaldi_ark', '--train_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', 'dump/raw/train_clean_100_sp/text,text,text', '--train_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/train/text_shape.bpe', '--valid_data_path_and_name_and_type', 'dump/raw/dev/text,text,text', '--valid_shape_file', 'exp/asr_stats_raw_en_bpe1000_sp/valid/text_shape.bpe', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/gpuadmin/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/home/gpuadmin/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of exp/asr_train_asr_raw_en_bpe1000_sp/train.log ###################
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (5): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (6): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (7): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (8): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (9): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (10): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (11): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (12): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (13): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (14): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (15): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (16): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (17): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (18): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (19): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (20): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (21): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (22): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (23): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (24): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (25): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (26): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (27): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
      (28): MambaCellBlock(
        (final_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        (modules_list): ModuleList(
          (0-2): 3 x IdentityModule()
          (3): ConvolutionModuleWrapper(
            (conv): ConvolutionModule(
              (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (depthwise_conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)
              (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (activation): Swish()
            )
          )
          (4): BiMambaModule(
            (mamba): Mamba(
              (in_proj): Linear(in_features=256, out_features=1024, bias=False)
              (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (act): SiLU()
              (x_proj): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj): Linear(in_features=16, out_features=512, bias=True)
              (conv1d_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
              (x_proj_b): Linear(in_features=512, out_features=48, bias=False)
              (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=256, bias=False)
            )
          )
        )
        (pre_norms): ModuleList(
          (0-2): 3 x Identity()
          (3-4): 2 x LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
    (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=256, out_features=1000, bias=True)
    (ctc_loss): CTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 22.10 M
    Number of trainable parameters: 22.10 M (100.0%)
    Size: 88.41 MB
    Type: torch.float32
[gpusystem] 2025-12-19 05:12:30,117 (abs_task:1459) INFO: Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.002
    lr: 2.5e-07
    maximize: False
    weight_decay: 1e-06
)
[gpusystem] 2025-12-19 05:12:30,117 (abs_task:1460) INFO: Scheduler: WarmupLR(warmup_steps=8000)
[gpusystem] 2025-12-19 05:12:30,117 (abs_task:1469) INFO: Saving the configuration in exp/asr_train_asr_raw_en_bpe1000_sp/config.yaml
[gpusystem] 2025-12-19 05:12:30,772 (abs_task:1885) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1419, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[gpusystem] 2025-12-19 05:12:30,773 (abs_task:1886) INFO: [train] mini-batch sizes summary: N-batch=1419, mean=60.3, min=26, max=337
[gpusystem] 2025-12-19 05:12:30,811 (read_text:31) INFO: keys_to_load is not None, only loading 85617 keys from dump/raw/train_clean_100_sp/text
[gpusystem] 2025-12-19 05:12:30,864 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[gpusystem] 2025-12-19 05:12:30,865 (abs_task:1910) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/train_clean_100_sp/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/train_clean_100_sp/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7fe8c51883d0>)
[gpusystem] 2025-12-19 05:12:30,886 (abs_task:1885) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=51, batch_bins=16000000, sort_in_batch=descending, sort_batch=descending)
[gpusystem] 2025-12-19 05:12:30,886 (abs_task:1886) INFO: [valid] mini-batch sizes summary: N-batch=51, mean=108.8, min=23, max=315
[gpusystem] 2025-12-19 05:12:30,888 (read_text:31) INFO: keys_to_load is not None, only loading 5551 keys from dump/raw/dev/text
[gpusystem] 2025-12-19 05:12:30,891 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[gpusystem] 2025-12-19 05:12:30,891 (abs_task:1910) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7fe8c0313100>)
[gpusystem] 2025-12-19 05:12:30,896 (abs_task:1885) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=5551, batch_size=1, key_file=exp/asr_stats_raw_en_bpe1000_sp/valid/speech_shape, 
[gpusystem] 2025-12-19 05:12:30,896 (abs_task:1886) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[gpusystem] 2025-12-19 05:12:30,898 (read_text:31) INFO: keys_to_load is not None, only loading 3 keys from dump/raw/dev/text
[gpusystem] 2025-12-19 05:12:30,900 (asr:512) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[gpusystem] 2025-12-19 05:12:30,900 (abs_task:1910) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "dump/raw/dev/wav.scp", "type": "kaldi_ark"}
  text: {"path": "dump/raw/dev/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7fe8c03425f0>)
/home/gpuadmin/espnet/espnet2/train/trainer.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
[gpusystem] 2025-12-19 05:12:30,902 (trainer:336) INFO: 1/100epoch started
Failed to import Flash Attention, using ESPnet default: No module named 'flash_attn'
/home/gpuadmin/espnet/espnet2/train/trainer.py:638: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
/home/gpuadmin/espnet/espnet2/asr/espnet_model.py:403: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(self.autocast_frontend, dtype=autocast_type):
[gpusystem] 2025-12-19 05:13:05,155 (trainer:811) INFO: 1epoch:train:1-70batch: iter_time=0.007, forward_time=0.171, loss_ctc=493.750, loss=493.750, backward_time=0.213, grad_norm=1.066e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.029, optim0_lr0=2.500e-06, train_time=1.965
[gpusystem] 2025-12-19 05:13:38,946 (trainer:811) INFO: 1epoch:train:71-140batch: iter_time=1.493e-04, forward_time=0.172, loss_ctc=479.628, loss=479.628, backward_time=0.216, grad_norm=2.458e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=6.875e-06, train_time=1.924
[gpusystem] 2025-12-19 05:14:12,288 (trainer:811) INFO: 1epoch:train:141-210batch: iter_time=1.376e-04, forward_time=0.168, loss_ctc=382.101, loss=382.101, backward_time=0.215, grad_norm=2.735e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.125e-05, train_time=1.910
[gpusystem] 2025-12-19 05:14:46,441 (trainer:811) INFO: 1epoch:train:211-280batch: iter_time=1.473e-04, forward_time=0.173, loss_ctc=243.857, loss=243.857, backward_time=0.219, grad_norm=2.414e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=1.563e-05, train_time=1.946
[gpusystem] 2025-12-19 05:15:20,668 (trainer:811) INFO: 1epoch:train:281-350batch: iter_time=1.450e-04, forward_time=0.172, loss_ctc=169.293, loss=169.293, backward_time=0.222, grad_norm=2.234e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.000e-05, train_time=1.962
[gpusystem] 2025-12-19 05:15:54,659 (trainer:811) INFO: 1epoch:train:351-420batch: iter_time=1.452e-04, forward_time=0.171, loss_ctc=126.956, loss=126.956, backward_time=0.218, grad_norm=1.377e+03, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.437e-05, train_time=1.937
[gpusystem] 2025-12-19 05:16:28,921 (trainer:811) INFO: 1epoch:train:421-490batch: iter_time=1.435e-04, forward_time=0.172, loss_ctc=94.432, loss=94.432, backward_time=0.222, grad_norm=139.469, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=2.875e-05, train_time=1.965
[gpusystem] 2025-12-19 05:17:03,529 (trainer:811) INFO: 1epoch:train:491-560batch: iter_time=1.292e-04, forward_time=0.174, loss_ctc=88.323, loss=88.323, backward_time=0.222, grad_norm=117.946, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=3.312e-05, train_time=1.970
[gpusystem] 2025-12-19 05:17:37,698 (trainer:811) INFO: 1epoch:train:561-630batch: iter_time=1.424e-04, forward_time=0.174, loss_ctc=96.543, loss=96.543, backward_time=0.219, grad_norm=82.254, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.011, optim0_lr0=3.750e-05, train_time=1.960
[gpusystem] 2025-12-19 05:18:11,533 (trainer:811) INFO: 1epoch:train:631-700batch: iter_time=1.309e-04, forward_time=0.170, loss_ctc=97.201, loss=97.201, backward_time=0.217, grad_norm=45.586, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.187e-05, train_time=1.927
[gpusystem] 2025-12-19 05:18:45,467 (trainer:811) INFO: 1epoch:train:701-770batch: iter_time=1.295e-04, forward_time=0.171, loss_ctc=89.149, loss=89.149, backward_time=0.218, grad_norm=59.929, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=4.625e-05, train_time=1.941
[gpusystem] 2025-12-19 05:19:20,030 (trainer:811) INFO: 1epoch:train:771-840batch: iter_time=1.390e-04, forward_time=0.174, loss_ctc=86.206, loss=86.206, backward_time=0.222, grad_norm=52.582, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.063e-05, train_time=1.972
[gpusystem] 2025-12-19 05:19:54,157 (trainer:811) INFO: 1epoch:train:841-910batch: iter_time=1.320e-04, forward_time=0.172, loss_ctc=86.007, loss=86.007, backward_time=0.220, grad_norm=35.876, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=5.500e-05, train_time=1.956
[gpusystem] 2025-12-19 05:20:28,216 (trainer:811) INFO: 1epoch:train:911-980batch: iter_time=1.401e-04, forward_time=0.172, loss_ctc=93.312, loss=93.312, backward_time=0.217, grad_norm=41.146, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.010, optim0_lr0=5.937e-05, train_time=1.941
[gpusystem] 2025-12-19 05:21:02,081 (trainer:811) INFO: 1epoch:train:981-1050batch: iter_time=1.385e-04, forward_time=0.170, loss_ctc=92.775, loss=92.775, backward_time=0.218, grad_norm=53.568, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=6.375e-05, train_time=1.938
[gpusystem] 2025-12-19 05:21:36,463 (trainer:811) INFO: 1epoch:train:1051-1120batch: iter_time=1.379e-04, forward_time=0.174, loss_ctc=88.737, loss=88.737, backward_time=0.220, grad_norm=59.954, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=6.813e-05, train_time=1.961
[gpusystem] 2025-12-19 05:22:10,144 (trainer:811) INFO: 1epoch:train:1121-1190batch: iter_time=1.413e-04, forward_time=0.170, loss_ctc=89.524, loss=89.524, backward_time=0.217, grad_norm=37.957, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=7.250e-05, train_time=1.930
[gpusystem] 2025-12-19 05:22:44,354 (trainer:811) INFO: 1epoch:train:1191-1260batch: iter_time=1.203e-04, forward_time=0.171, loss_ctc=89.158, loss=89.158, backward_time=0.220, grad_norm=42.462, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=7.688e-05, train_time=1.948
[gpusystem] 2025-12-19 05:23:18,163 (trainer:811) INFO: 1epoch:train:1261-1330batch: iter_time=1.314e-04, forward_time=0.171, loss_ctc=90.791, loss=90.791, backward_time=0.216, grad_norm=46.064, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.125e-05, train_time=1.939
[gpusystem] 2025-12-19 05:23:52,310 (trainer:811) INFO: 1epoch:train:1331-1400batch: iter_time=1.254e-04, forward_time=0.170, loss_ctc=87.981, loss=87.981, backward_time=0.219, grad_norm=46.936, clip=100.000, loss_scale=6.554e+04, optim_step_time=0.009, optim0_lr0=8.562e-05, train_time=1.943
/home/gpuadmin/espnet/espnet2/train/trainer.py:856: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(
 Traceback (most recent call last) 
 /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py:196 in          
 _run_module_as_main                                                          
                                                                              
   193    main_globals = sys.modules["__main__"].__dict__                    
   194    if alter_argv:                                                     
   195       sys.argv[0] = mod_spec.origin                                  
  196    return _run_code(code, main_globals, None,                         
   197                 "__main__", mod_spec)                             
   198                                                                        
   199 def run_module(mod_name, init_globals=None,                            
                                                                              
 /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/runpy.py:86 in _run_code 
                                                                              
    83                   __loader__ = loader,                            
    84                   __package__ = pkg_name,                         
    85                   __spec__ = mod_spec)                            
   86    exec(code, run_globals)                                            
    87    return run_globals                                                 
    88                                                                        
    89 def _run_module_code(code, init_globals=None,                          
                                                                              
 /home/gpuadmin/espnet/espnet2/bin/asr_train.py:23 in <module>                
                                                                              
   20                                                                         
   21                                                                         
   22 if __name__ == "__main__":                                              
  23    main()                                                              
   24                                                                         
                                                                              
 /home/gpuadmin/espnet/espnet2/bin/asr_train.py:19 in main                    
                                                                              
   16             > conf/train_asr.yaml                                   
   17       % python asr_train.py --config conf/train_asr.yaml              
   18    """                                                                 
  19    ASRTask.main(cmd=cmd)                                               
   20                                                                         
   21                                                                         
   22 if __name__ == "__main__":                                              
                                                                              
 /home/gpuadmin/espnet/espnet2/tasks/abs_task.py:1281 in main                 
                                                                              
   1278       # "distributed" is decided using the other command args       
   1279       resolve_distributed_mode(args)                                
   1280       if not args.distributed or not args.multiprocessing_distribut 
  1281          cls.main_worker(args)                                     
   1282                                                                     
   1283       else:                                                         
   1284          assert args.ngpu > 1, args.ngpu                           
                                                                              
 /home/gpuadmin/espnet/espnet2/tasks/abs_task.py:1638 in main_worker          
                                                                              
   1635                distributed_option.init_deepspeed()               
   1636                                                                    
   1637          trainer_options = cls.trainer.build_options(args)         
  1638          cls.trainer.run(                                          
   1639             model=model,                                          
   1640             optimizers=optimizers,                                
   1641             schedulers=schedulers,                                
                                                                              
 /home/gpuadmin/espnet/espnet2/train/trainer.py:357 in run                    
                                                                              
   354                                                                     
   355          torch.cuda.empty_cache()                                   
   356          with reporter.observe("valid") as sub_reporter:            
  357             cls.validate_one_epoch(                                
   358                model=dp_model,                                    
   359                iterator=valid_iter_factory.build_iter(iepoch),    
   360                reporter=sub_reporter,                             
                                                                              
 /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/util 
 s/_contextlib.py:120 in decorate_context                                     
                                                                              
   117    @functools.wraps(func)                                             
   118    def decorate_context(*args, **kwargs):                             
   119       with ctx_factory():                                            
  120          return func(*args, **kwargs)                               
   121                                                                       
   122    return decorate_context                                            
   123                                                                        
                                                                              
 /home/gpuadmin/espnet/espnet2/train/trainer.py:860 in validate_one_epoch     
                                                                              
   857             options.use_amp,                                       
   858             **autocast_args,                                       
   859          ):                                                         
  860             retval = model(**batch)                                
   861                                                                     
   862          if isinstance(retval, dict):                               
   863             stats = retval["stats"]                                
                                                                              
 /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m 
 odules/module.py:1775 in _wrapped_call_impl                                  
                                                                              
   1772       if self._compiled_call_impl is not None:                      
   1773          return self._compiled_call_impl(*args, **kwargs)  # type: 
   1774       else:                                                         
  1775          return self._call_impl(*args, **kwargs)                   
   1776                                                                      
   1777    # torchrec tests the code consistency with the following code     
   1778    # fmt: off                                                        
                                                                              
 /home/gpuadmin/anaconda3/envs/junghc/lib/python3.10/site-packages/torch/nn/m 
 odules/module.py:1786 in _call_impl                                          
                                                                              
   1783       if not (self._backward_hooks or self._backward_pre_hooks or s 
   1784             or _global_backward_pre_hooks or _global_backward_hoo 
   1785             or _global_forward_hooks or _global_forward_pre_hooks 
  1786          return forward_call(*args, **kwargs)                      
   1787                                                                     
   1788       result = None                                                 
   1789       called_always_called_hooks = set()                            
                                                                              
 /home/gpuadmin/espnet/espnet2/asr/espnet_model.py:271 in forward             
                                                                              
   268                                                                      
   269       # 1. CTC branch                                                
   270       if self.ctc_weight != 0.0:                                     
  271          loss_ctc, cer_ctc, wer_ctc = self._calc_ctc_loss(          
   272             encoder_out, encoder_out_lens, text, text_lengths      
   273          )                                                          
   274                                                                        
                                                                              
 /home/gpuadmin/espnet/espnet2/asr/espnet_model.py:625 in _calc_ctc_loss      
                                                                              
   622       wer_ctc = None                                                 
   623       if not self.training and self.error_calculator is not None:    
   624          ys_hat = self.ctc.argmax(encoder_out).data                 
  625          cer_ctc, wer_ctc = self.error_calculator(ys_hat.cpu(), ys_ 
   626       return loss_ctc, cer_ctc, wer_ctc                              
   627                                                                       
   628    def _calc_transducer_loss(                                         

TypeError: cannot unpack non-iterable float object
# Accounting: time=701 threads=1
# Ended (code 1) at Fri Dec 19 05:24:04 UTC 2025, elapsed time 701 seconds

